{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbJtdDUGEyq3",
        "outputId": "0316f6f1-18a8-498e-8d27-e6f4631115aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting face_recognition\n",
            "  Using cached face_recognition-1.3.0-py2.py3-none-any.whl.metadata (21 kB)\n",
            "Collecting face-recognition-models>=0.3.0 (from face_recognition)\n",
            "  Using cached face_recognition_models-0.3.0.tar.gz (100.1 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.11/dist-packages (from face_recognition) (8.1.8)\n",
            "Requirement already satisfied: dlib>=19.7 in /usr/local/lib/python3.11/dist-packages (from face_recognition) (19.24.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from face_recognition) (1.26.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from face_recognition) (11.1.0)\n",
            "Downloading face_recognition-1.3.0-py2.py3-none-any.whl (15 kB)\n",
            "Building wheels for collected packages: face-recognition-models\n",
            "  Building wheel for face-recognition-models (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for face-recognition-models: filename=face_recognition_models-0.3.0-py2.py3-none-any.whl size=100566162 sha256=156b331755acb996574fc0eaea66385376550cbd66344411be8a9776014b9fa8\n",
            "  Stored in directory: /root/.cache/pip/wheels/04/52/ec/9355da79c29f160b038a20c784db2803c2f9fa2c8a462c176a\n",
            "Successfully built face-recognition-models\n",
            "Installing collected packages: face-recognition-models, face_recognition\n",
            "Successfully installed face-recognition-models-0.3.0 face_recognition-1.3.0\n"
          ]
        }
      ],
      "source": [
        "# Tải mô hình phát hiện khuôn\n",
        "!pip install face_recognition\n",
        "import face_recognition\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "from tensorflow.keras.models import load_model\n"
      ],
      "metadata": {
        "id": "VFm0gJOBE1PN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_SIZE = [224, 224, 3]  # kích thước đầu vào\n",
        "\n",
        "PIXEL_MEAN = [103.939, 116.779, 123.68] # BGR pixel mean value\n",
        "\n",
        "NUM_CLASSES = 2           # số lượng lớp đầu ra\n",
        "\n",
        "BKG_LABEL = 0             #\n",
        "\n",
        "LEARNING_RATE = 0.001     # tốc độ học - xác định tốc độ cập nhật các trọng số trong quá trình tối ưu hóa mô hình.\n",
        "\n",
        "DECAY_RATE = 0.95         # Tốc độ học sẽ bị giảm sau mốc sự kiện nhất định\n",
        "\n",
        "NUM_EPOCHS = 20           # Một epoch đại diện cho một lần model đã được huấn luyện trên tất cả các mẫu trong tập dữ liệu huấn luyện\n",
        "\n",
        "TRAIN_BATCH_SIZE = 64     # Dữ liệu huấn luyện thường được chia thành các batch (nhóm) để tăng tốc độ huấn luyện và tối ưu hóa\n",
        "\n",
        "\n",
        "MODEL_DIR_PREFIX = 'ckpt' #\n",
        "\n",
        "MODEL_NAME = 'model'      #\n",
        "\n",
        "TEST_BATCH_SIZE = 8       #"
      ],
      "metadata": {
        "id": "5dtVRGerE3tj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = tf.keras.applications.VGG16(\n",
        "    weights=None,             # Mô hình chưa được đào tạo trước\n",
        "    include_top=False,        # base_model không bao gồm các lớp Fully Connected\n",
        "    input_shape=IMG_SIZE\n",
        ")\n",
        "\n",
        "\n",
        "# Thêm các tầng phân loại mới\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(base_model)\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
        "    # ngẫu nhiên \"loại bỏ\" một phần các đơn vị đầu vào trong quá trình huấn luyện\n",
        "    # giảm hiện tượng overfitting\n",
        "model.add(tf.keras.layers.Dropout(0.5))\n",
        "model.add(tf.keras.layers.Dense(NUM_CLASSES, activation='softmax'))\n"
      ],
      "metadata": {
        "id": "eBOffyxrFAn-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Sau decay_steps, giảm tỷ lệ học một lần\n",
        "decay_steps = 1000\n",
        "\n",
        "# Tạo lịch trình học tập tùy chỉnh\n",
        "learning_rate_schedule = tf.optimizers.schedules.PiecewiseConstantDecay(\n",
        "    boundaries=[decay_steps],\n",
        "    values=[LEARNING_RATE, LEARNING_RATE * DECAY_RATE]\n",
        ")\n",
        "\n",
        "# Xác định trình tối ưu hóa với lịch trình học tập tùy chỉnh\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate_schedule, momentum=0.9)\n",
        "\n",
        "# Compile mô hình\n",
        "model.compile(optimizer=optimizer,\n",
        "              # Tính toán sự khác biệt giữa xác xuất dự đoán và thực tế\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "gik2RsWCFEqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chuẩn bị dữ liệu huấn luyện\n",
        "def convert_positive_to_negative_example(image_path):\n",
        "    image = cv2.imread(image_path)\n",
        "    # Dùng face_recognition để phát hiện khuôn mặt và các điểm đặc trưng\n",
        "    face_landmarks_list = face_recognition.face_landmarks(image)\n",
        "\n",
        "# Kiểm tra xem có khuôn mặt nào được phát hiện hay không\n",
        "    result = image\n",
        "    for i in range(0, len(face_landmarks_list)):\n",
        "        # Lấy danh sách các điểm đặc trưng của khuôn mặt đầu tiên\n",
        "        face_landmarks = face_landmarks_list[i]\n",
        "\n",
        "        # Lấy các điểm đặc trưng của khuôn mặt\n",
        "        eyebrow_left = face_landmarks['left_eyebrow']\n",
        "        eyebrow_right = face_landmarks['right_eyebrow']\n",
        "        eye_left = face_landmarks['left_eye']\n",
        "        eye_right = face_landmarks['right_eye']\n",
        "        nose = face_landmarks['nose_tip']\n",
        "        lip = face_landmarks['bottom_lip']\n",
        "\n",
        "    # cắt hình\n",
        "        # Tính toán tọa độ góc trái trên của hình ảnh mới\n",
        "        top_left_x = min(eyebrow_left[0][0], eye_left[0][0], nose[0][0], lip[6][0])\n",
        "        top_left_y = image.shape[:2][1]\n",
        "        for i in range(0, 5):\n",
        "            top_left_y = min(top_left_y, eyebrow_left[i][1], eyebrow_right[i][1])\n",
        "\n",
        "        # Tính toán tọa độ góc phải dưới của hình ảnh mới\n",
        "        bottom_right_x = max(eyebrow_right[-1][0], eye_right[3][0], nose[0][0], lip[0][0])\n",
        "        bottom_right_y = 0\n",
        "        for i in range(0, 12):\n",
        "            bottom_right_y = max(bottom_right_y, lip[i][1])\n",
        "\n",
        "        #xét điều kiện tránh cắt ra ngoài ảnh\n",
        "        top_left_x = max(top_left_x, 0)\n",
        "        top_left_y = max(top_left_y, 0)\n",
        "        bottom_right_x = min(bottom_right_x, image.shape[:2][0])\n",
        "        bottom_right_y = min(bottom_right_y, image.shape[:2][1])\n",
        "\n",
        "        cropped_image = image[top_left_y:bottom_right_y, top_left_x:bottom_right_x]\n",
        "        cropped_image_size = cropped_image.shape[:2]\n",
        "\n",
        "        # Phóng to hoặc thu nhỏ ảnh ngẫu nhiên\n",
        "        scale_factor = random.uniform(0.5, 1.5)  # Tính toán tỷ lệ ngẫu nhiên\n",
        "        resized_image = cv2.resize(cropped_image, None, fx=scale_factor, fy=scale_factor)\n",
        "\n",
        "        # Ghép lại ảnh ban đầu\n",
        "        resized_image = cv2.resize(resized_image, (cropped_image_size[1], cropped_image_size[0])) # Trả lại kích thước ban đầu cho ảnh đã cắt\n",
        "        image[top_left_y:bottom_right_y, top_left_x:bottom_right_x] = resized_image  # Đặt ảnh đã bị cắt vào vị trí ảnh cắt lúc đầu trong ảnh gốc\n",
        "\n",
        "    #Áp dụng làm mờ cho vùng mặt nạ được vẽ gồm lông mày và môi dưới\n",
        "        # Tạo mặt nạ\n",
        "        mask_points = np.concatenate((eyebrow_right[-1], lip[0], lip[1], lip[2], lip[3], lip[4], lip[5], lip[6], eyebrow_left[0]), axis=0).reshape(-1, 2)\n",
        "\n",
        "        for i in range(1, 5):\n",
        "            if eyebrow_left[i-1][1] > eyebrow_left[i][1]:\n",
        "                mask_points = np.concatenate((mask_points, np.expand_dims(eyebrow_left[i], axis=0)), axis=0)\n",
        "\n",
        "        for i in range(1, 5):\n",
        "            if eyebrow_right[-i][1] > eyebrow_right[-i-1][1]:\n",
        "                mask_points = np.concatenate((np.expand_dims(eyebrow_right[-i-1], axis=0), mask_points), axis=0)\n",
        "\n",
        "        # Tạo mask ban đầu\n",
        "        mask = np.zeros(image.shape[:2], dtype=np.uint8)\n",
        "        cv2.fillPoly(mask, [mask_points.astype(np.int32)], 255)\n",
        "\n",
        "        # Mở rộng mask để cắt thêm phần xung quanh\n",
        "        kernel_size = 5  # Kích thước kernel dilation\n",
        "        border_size = 10  # Kích thước viền\n",
        "        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (kernel_size, kernel_size))\n",
        "        expanded_mask = cv2.dilate(mask, kernel, iterations=border_size)\n",
        "\n",
        "        # Cắt vùng đã chọn từ hình ảnh gốc\n",
        "        face_cut = cv2.bitwise_and(image, image, mask=expanded_mask)\n",
        "\n",
        "        # Áp dụng hiệu ứng làm mờ Gaussian cho vùng đã được cắt ra\n",
        "        blurred_face = cv2.GaussianBlur(face_cut, (5, 5), 0)\n",
        "\n",
        "        mask = np.expand_dims(mask, axis=2)\n",
        "        result = np.where(mask == 0, image, blurred_face)\n",
        "    return result\n"
      ],
      "metadata": {
        "id": "ntbilYz1FGQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tạo ra batch\n",
        "\n",
        "def batch_generator(data_dir, batch_size):\n",
        "    file_list = os.listdir(data_dir)\n",
        "    num_samples = len(file_list)\n",
        "    num_batches = num_samples // batch_size\n",
        "    batches = []\n",
        "    def generator():\n",
        "        for batch_index in range(num_batches):\n",
        "            start_index = batch_index * batch_size\n",
        "            end_index = (batch_index + 1) * batch_size\n",
        "            batch_files = file_list[start_index:end_index]\n",
        "\n",
        "            x_batch = []\n",
        "            y_batch = []\n",
        "\n",
        "            # Lấy ngẫu nhiên nửa số lượng ví dụ tích cực\n",
        "            positive_examples = np.random.choice(batch_files, size=batch_size//2, replace=False)\n",
        "\n",
        "            for file_name in batch_files:\n",
        "                # Đường dẫn đầy đủ của hình ảnh\n",
        "                image_path = os.path.join(data_dir, file_name)\n",
        "                image = cv2.imread(image_path)\n",
        "\n",
        "                if file_name not in positive_examples and len(face_recognition.face_locations(image)) > 0:\n",
        "                    image = convert_positive_to_negative_example(image_path)\n",
        "                    # Tiền xử lý ảnh\n",
        "                    # Thay đổi kích thước ảnh\n",
        "                    image = cv2.resize(image, (224, 224))\n",
        "\n",
        "                    # Chuẩn hóa pixel\n",
        "                    image -= np.array(PIXEL_MEAN, dtype=np.uint8)\n",
        "                    label = 1\n",
        "                    x_batch.append(image)\n",
        "                    y_batch.append(label)\n",
        "                else :\n",
        "                    # Tiền xử lý ảnh\n",
        "                    # Thay đổi kích thước ảnh\n",
        "                    image = cv2.resize(image, (224, 224))\n",
        "\n",
        "                    # Chuẩn hóa pixel\n",
        "                    image -= np.array(PIXEL_MEAN, dtype=np.uint8)\n",
        "                    label = 0\n",
        "                    x_batch.append(image)\n",
        "                    y_batch.append(label)\n",
        "\n",
        "            x_batch = np.array(x_batch)\n",
        "            y_batch = tf.keras.utils.to_categorical(y_batch, num_classes=2)\n",
        "\n",
        "            yield x_batch, y_batch\n",
        "\n",
        "    dataset = tf.data.Dataset.from_generator(generator, output_signature=(\n",
        "        tf.TensorSpec(shape=(None, 224, 224, 3), dtype=tf.uint8),\n",
        "        tf.TensorSpec(shape=(None, 2), dtype=tf.float32)\n",
        "    ))\n",
        "\n",
        "    return dataset\n",
        "# Tạo dataset cho đánh giá\n",
        "eval_data_dir = \"/content/drive/MyDrive/eval_data\"\n",
        "eval_dataset = batch_generator(eval_data_dir, TEST_BATCH_SIZE)\n"
      ],
      "metadata": {
        "id": "-PuMH7KNFHv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "HOqqruu5uXnK",
        "outputId": "978e4fce-0191-468c-ce76-7a9aa80f7cae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ vgg16 (\u001b[38;5;33mFunctional\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m512\u001b[0m)           │      \u001b[38;5;34m14,714,688\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25088\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │       \u001b[38;5;34m6,422,784\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                   │             \u001b[38;5;34m514\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ vgg16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)           │      <span style=\"color: #00af00; text-decoration-color: #00af00\">14,714,688</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25088</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │       <span style=\"color: #00af00; text-decoration-color: #00af00\">6,422,784</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">514</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m21,137,986\u001b[0m (80.64 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">21,137,986</span> (80.64 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m21,137,986\u001b[0m (80.64 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">21,137,986</span> (80.64 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_data_dir = \"/content/drive/MyDrive/eval_data\"\n",
        "eval_dataset = batch_generator(eval_data_dir, 8)\n",
        "\n",
        "batch_size = 64\n",
        "data_dir = \"/content/drive/MyDrive/positive_example\"\n",
        "\n",
        "num_epochs = 10\n",
        "batch_size = 64\n",
        "loss_values = []\n",
        "accuracy_values = []\n",
        "best_accuracy = 0.0  # Biến để theo dõi độ chính xác tốt nhất\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for x_batch, y_batch in batch_generator(data_dir, batch_size):\n",
        "        model.train_on_batch(x_batch, y_batch)\n",
        "\n",
        "    # Đánh giá mô hình trên tập validation\n",
        "    val_loss, val_accuracy = model.evaluate(eval_dataset)\n",
        "\n",
        "    loss_values.append(val_loss)\n",
        "    accuracy_values.append(val_accuracy)\n",
        "\n",
        "    # In thông tin về epoch\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} - val_loss: {val_loss:.4f} - val_accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "    # Kiểm tra nếu độ chính xác hiện tại tốt hơn độ chính xác tốt nhất\n",
        "    if val_accuracy > best_accuracy:\n",
        "        best_accuracy = val_accuracy\n",
        "        # Lưu lại trạng thái của mô hình\n",
        "        model.save_weights(\"/content/drive/MyDrive/best_model.h5\")\n",
        "        model.save(\"/content/drive/MyDrive/Model.h5\")\n"
      ],
      "metadata": {
        "id": "EaMxxagPFJUZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d93f02c6-57de-41ff-b03f-c9ac667d0bb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "{{function_node __wrapped__IteratorGetNext_output_types_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} TypeError: __call__(): incompatible function arguments. The following argument types are supported:\n    1. (self: _dlib_pybind11.fhog_object_detector, image: numpy.ndarray, upsample_num_times: int = 0) -> _dlib_pybind11.rectangles\n\nInvoked with: <_dlib_pybind11.fhog_object_detector object at 0x7a6eecce9cf0>, None, 1\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/script_ops.py\", line 270, in __call__\n    ret = func(*args)\n          ^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/from_generator_op.py\", line 198, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"<ipython-input-8-3fda35546780>\", line 25, in generator\n    if file_name not in positive_examples and len(face_recognition.face_locations(image)) > 0:\n                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.11/dist-packages/face_recognition/api.py\", line 121, in face_locations\n    return [_trim_css_to_bounds(_rect_to_css(face), img.shape) for face in _raw_face_locations(img, number_of_times_to_upsample, model)]\n                                                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.11/dist-packages/face_recognition/api.py\", line 105, in _raw_face_locations\n    return face_detector(img, number_of_times_to_upsample)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nTypeError: __call__(): incompatible function arguments. The following argument types are supported:\n    1. (self: _dlib_pybind11.fhog_object_detector, image: numpy.ndarray, upsample_num_times: int = 0) -> _dlib_pybind11.rectangles\n\nInvoked with: <_dlib_pybind11.fhog_object_detector object at 0x7a6eecce9cf0>, None, 1\n\n\n\t [[{{node PyFunc}}]] [Op:IteratorGetNext] name: ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-df56b8160a5c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    824\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 826\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    827\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    774\u001b[0m     \u001b[0;31m# to communicate that there is no more data to iterate over.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecution_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSYNC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 776\u001b[0;31m       ret = gen_dataset_ops.iterator_get_next(\n\u001b[0m\u001b[1;32m    777\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m           \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   3084\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3085\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3086\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3087\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3088\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   5981\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNoReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5982\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5983\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5984\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__IteratorGetNext_output_types_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} TypeError: __call__(): incompatible function arguments. The following argument types are supported:\n    1. (self: _dlib_pybind11.fhog_object_detector, image: numpy.ndarray, upsample_num_times: int = 0) -> _dlib_pybind11.rectangles\n\nInvoked with: <_dlib_pybind11.fhog_object_detector object at 0x7a6eecce9cf0>, None, 1\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/script_ops.py\", line 270, in __call__\n    ret = func(*args)\n          ^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/from_generator_op.py\", line 198, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"<ipython-input-8-3fda35546780>\", line 25, in generator\n    if file_name not in positive_examples and len(face_recognition.face_locations(image)) > 0:\n                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.11/dist-packages/face_recognition/api.py\", line 121, in face_locations\n    return [_trim_css_to_bounds(_rect_to_css(face), img.shape) for face in _raw_face_locations(img, number_of_times_to_upsample, model)]\n                                                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.11/dist-packages/face_recognition/api.py\", line 105, in _raw_face_locations\n    return face_detector(img, number_of_times_to_upsample)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nTypeError: __call__(): incompatible function arguments. The following argument types are supported:\n    1. (self: _dlib_pybind11.fhog_object_detector, image: numpy.ndarray, upsample_num_times: int = 0) -> _dlib_pybind11.rectangles\n\nInvoked with: <_dlib_pybind11.fhog_object_detector object at 0x7a6eecce9cf0>, None, 1\n\n\n\t [[{{node PyFunc}}]] [Op:IteratorGetNext] name: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Đường dẫn dữ liệu đánh giá và huấn luyện\n",
        "train_data_dir = \"/content/drive/My Drive/dataset_evaluation/Dataset/Train\"\n",
        "\n",
        "\n",
        "# Huấn luyện mô hình\n",
        "num_epochs = 20\n",
        "loss_values = []\n",
        "accuracy_values = []\n",
        "best_accuracy = 0.0  # Biến để theo dõi độ chính xác tốt nhất\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for x_batch, y_batch in batch_generator(train_data_dir, TRAIN_BATCH_SIZE):\n",
        "        model.train_on_batch(x_batch, y_batch)\n",
        "\n",
        "    # Đánh giá mô hình trên tập validation\n",
        "    val_loss, val_accuracy = model.evaluate(eval_dataset)\n",
        "\n",
        "    loss_values.append(val_loss)\n",
        "    accuracy_values.append(val_accuracy)\n",
        "\n",
        "    # In thông tin về epoch\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} - val_loss: {val_loss:.4f} - val_accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "    # Kiểm tra nếu độ chính xác hiện tại tốt hơn độ chính xác tốt nhất\n",
        "    if val_accuracy > best_accuracy:\n",
        "        best_accuracy = val_accuracy\n",
        "        # Lưu lại trạng thái của mô hình\n",
        "        model.save(\"/content/drive/MyDrive/Model_23012025.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wB-Fx7b3_dB4",
        "outputId": "2ebd0f09-491e-4818-ed86-8aeb253e62ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50/50 [==============================] - 36s 522ms/step - loss: 0.0583 - accuracy: 0.9825\n",
            "Epoch 1/20 - val_loss: 0.0583 - val_accuracy: 0.9825\n",
            "50/50 [==============================] - 27s 534ms/step - loss: 0.0507 - accuracy: 0.9800\n",
            "Epoch 2/20 - val_loss: 0.0507 - val_accuracy: 0.9800\n",
            "50/50 [==============================] - 28s 543ms/step - loss: 0.0764 - accuracy: 0.9700\n",
            "Epoch 3/20 - val_loss: 0.0764 - val_accuracy: 0.9700\n",
            "50/50 [==============================] - 25s 498ms/step - loss: 0.0813 - accuracy: 0.9700\n",
            "Epoch 4/20 - val_loss: 0.0813 - val_accuracy: 0.9700\n",
            "50/50 [==============================] - 25s 502ms/step - loss: 0.0628 - accuracy: 0.9800\n",
            "Epoch 5/20 - val_loss: 0.0628 - val_accuracy: 0.9800\n",
            "50/50 [==============================] - 25s 494ms/step - loss: 0.0735 - accuracy: 0.9700\n",
            "Epoch 6/20 - val_loss: 0.0735 - val_accuracy: 0.9700\n",
            "50/50 [==============================] - 24s 482ms/step - loss: 0.0895 - accuracy: 0.9750\n",
            "Epoch 7/20 - val_loss: 0.0895 - val_accuracy: 0.9750\n",
            "50/50 [==============================] - 24s 487ms/step - loss: 0.0654 - accuracy: 0.9800\n",
            "Epoch 8/20 - val_loss: 0.0654 - val_accuracy: 0.9800\n",
            "50/50 [==============================] - 25s 508ms/step - loss: 0.0843 - accuracy: 0.9650\n",
            "Epoch 9/20 - val_loss: 0.0843 - val_accuracy: 0.9650\n",
            "50/50 [==============================] - 24s 488ms/step - loss: 0.0811 - accuracy: 0.9725\n",
            "Epoch 10/20 - val_loss: 0.0811 - val_accuracy: 0.9725\n",
            "50/50 [==============================] - 25s 505ms/step - loss: 0.0677 - accuracy: 0.9725\n",
            "Epoch 11/20 - val_loss: 0.0677 - val_accuracy: 0.9725\n",
            "50/50 [==============================] - 25s 507ms/step - loss: 0.0665 - accuracy: 0.9750\n",
            "Epoch 12/20 - val_loss: 0.0665 - val_accuracy: 0.9750\n",
            "50/50 [==============================] - 24s 481ms/step - loss: 0.0713 - accuracy: 0.9750\n",
            "Epoch 13/20 - val_loss: 0.0713 - val_accuracy: 0.9750\n",
            "50/50 [==============================] - 24s 468ms/step - loss: 0.0746 - accuracy: 0.9725\n",
            "Epoch 14/20 - val_loss: 0.0746 - val_accuracy: 0.9725\n",
            "50/50 [==============================] - 24s 484ms/step - loss: 0.0800 - accuracy: 0.9700\n",
            "Epoch 15/20 - val_loss: 0.0800 - val_accuracy: 0.9700\n",
            "50/50 [==============================] - 24s 475ms/step - loss: 0.0772 - accuracy: 0.9750\n",
            "Epoch 16/20 - val_loss: 0.0772 - val_accuracy: 0.9750\n",
            "50/50 [==============================] - 32s 636ms/step - loss: 0.0990 - accuracy: 0.9650\n",
            "Epoch 17/20 - val_loss: 0.0990 - val_accuracy: 0.9650\n",
            "50/50 [==============================] - 24s 477ms/step - loss: 0.0824 - accuracy: 0.9675\n",
            "Epoch 18/20 - val_loss: 0.0824 - val_accuracy: 0.9675\n",
            "50/50 [==============================] - 25s 497ms/step - loss: 0.0751 - accuracy: 0.9750\n",
            "Epoch 19/20 - val_loss: 0.0751 - val_accuracy: 0.9750\n",
            "50/50 [==============================] - 25s 502ms/step - loss: 0.0808 - accuracy: 0.9625\n",
            "Epoch 20/20 - val_loss: 0.0808 - val_accuracy: 0.9625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Vẽ biểu đồ loss của train và validation\n",
        "plt.plot(range(1, 2*num_epochs+1), loss_values, label='Train Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss over Epochs')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Vẽ biểu đồ độ chính xác của train và validation\n",
        "plt.plot(range(1, 2*num_epochs+1), accuracy_values, label='Train Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy over Epochs')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bwu9UVu-FTQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nháp\n"
      ],
      "metadata": {
        "id": "-MlmngYWklvA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tải mô hình đã huấn luyện nếu có\n",
        "model_path = '/content/drive/MyDrive/Model.h5'\n",
        "if os.path.exists(model_path):\n",
        "    model = load_model(model_path)\n",
        "\n",
        "# Đường dẫn dữ liệu đánh giá và huấn luyện\n",
        "train_data_dir = \"/content/drive/My Drive/dataset_evaluation/Dataset/Train\"\n",
        "\n",
        "\n",
        "# Huấn luyện mô hình\n",
        "num_epochs = 20\n",
        "loss_values = []\n",
        "accuracy_values = []\n",
        "best_accuracy = 0.0  # Biến để theo dõi độ chính xác tốt nhất\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for x_batch, y_batch in batch_generator(train_data_dir, TRAIN_BATCH_SIZE):\n",
        "        model.train_on_batch(x_batch, y_batch)\n",
        "\n",
        "    # Đánh giá mô hình trên tập validation\n",
        "    val_loss, val_accuracy = model.evaluate(eval_dataset)\n",
        "\n",
        "    loss_values.append(val_loss)\n",
        "    accuracy_values.append(val_accuracy)\n",
        "\n",
        "    # In thông tin về epoch\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} - val_loss: {val_loss:.4f} - val_accuracy: {val_accuracy:.4f}\")\n",
        "    model.save_weights(\"/content/drive/MyDrive/best_model.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0noOSqksClKD",
        "outputId": "caa241c9-96bf-4ff2-90e7-6632ed6c199d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50/50 [==============================] - 25s 491ms/step - loss: 0.0656 - accuracy: 0.9775\n",
            "Epoch 1/20 - val_loss: 0.0656 - val_accuracy: 0.9775\n",
            "50/50 [==============================] - 24s 478ms/step - loss: 0.0666 - accuracy: 0.9750\n",
            "Epoch 2/20 - val_loss: 0.0666 - val_accuracy: 0.9750\n",
            "50/50 [==============================] - 25s 491ms/step - loss: 0.0622 - accuracy: 0.9825\n",
            "Epoch 3/20 - val_loss: 0.0622 - val_accuracy: 0.9825\n",
            "50/50 [==============================] - 24s 479ms/step - loss: 0.0840 - accuracy: 0.9700\n",
            "Epoch 4/20 - val_loss: 0.0840 - val_accuracy: 0.9700\n",
            "50/50 [==============================] - 26s 528ms/step - loss: 0.0686 - accuracy: 0.9725\n",
            "Epoch 5/20 - val_loss: 0.0686 - val_accuracy: 0.9725\n",
            "50/50 [==============================] - 25s 502ms/step - loss: 0.0801 - accuracy: 0.9700\n",
            "Epoch 6/20 - val_loss: 0.0801 - val_accuracy: 0.9700\n",
            "50/50 [==============================] - 25s 493ms/step - loss: 0.0578 - accuracy: 0.9800\n",
            "Epoch 7/20 - val_loss: 0.0578 - val_accuracy: 0.9800\n",
            "50/50 [==============================] - 25s 501ms/step - loss: 0.0669 - accuracy: 0.9800\n",
            "Epoch 8/20 - val_loss: 0.0669 - val_accuracy: 0.9800\n",
            "50/50 [==============================] - 27s 522ms/step - loss: 0.0505 - accuracy: 0.9850\n",
            "Epoch 9/20 - val_loss: 0.0505 - val_accuracy: 0.9850\n",
            "50/50 [==============================] - 25s 501ms/step - loss: 0.0878 - accuracy: 0.9625\n",
            "Epoch 10/20 - val_loss: 0.0878 - val_accuracy: 0.9625\n",
            "50/50 [==============================] - 25s 494ms/step - loss: 0.0880 - accuracy: 0.9650\n",
            "Epoch 11/20 - val_loss: 0.0880 - val_accuracy: 0.9650\n",
            "50/50 [==============================] - 25s 507ms/step - loss: 0.0920 - accuracy: 0.9600\n",
            "Epoch 12/20 - val_loss: 0.0920 - val_accuracy: 0.9600\n",
            "50/50 [==============================] - 25s 502ms/step - loss: 0.0860 - accuracy: 0.9700\n",
            "Epoch 13/20 - val_loss: 0.0860 - val_accuracy: 0.9700\n",
            "50/50 [==============================] - 24s 473ms/step - loss: 0.0543 - accuracy: 0.9825\n",
            "Epoch 14/20 - val_loss: 0.0543 - val_accuracy: 0.9825\n",
            "50/50 [==============================] - 25s 486ms/step - loss: 0.0770 - accuracy: 0.9675\n",
            "Epoch 15/20 - val_loss: 0.0770 - val_accuracy: 0.9675\n",
            "50/50 [==============================] - 25s 499ms/step - loss: 0.0829 - accuracy: 0.9675\n",
            "Epoch 16/20 - val_loss: 0.0829 - val_accuracy: 0.9675\n",
            "50/50 [==============================] - 25s 499ms/step - loss: 0.0843 - accuracy: 0.9600\n",
            "Epoch 17/20 - val_loss: 0.0843 - val_accuracy: 0.9600\n",
            "50/50 [==============================] - 25s 503ms/step - loss: 0.0823 - accuracy: 0.9700\n",
            "Epoch 18/20 - val_loss: 0.0823 - val_accuracy: 0.9700\n",
            "50/50 [==============================] - 26s 516ms/step - loss: 0.0598 - accuracy: 0.9775\n",
            "Epoch 19/20 - val_loss: 0.0598 - val_accuracy: 0.9775\n",
            "50/50 [==============================] - 25s 499ms/step - loss: 0.0943 - accuracy: 0.9650\n",
            "Epoch 20/20 - val_loss: 0.0943 - val_accuracy: 0.9650\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/inpainting.zip -d /content/data_test\n",
        "\n",
        "!unzip /content/82.zip -d /content/82\n",
        "\n"
      ],
      "metadata": {
        "id": "-giG6r_n5sGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "\n",
        "# Đường dẫn thư mục và mô hình\n",
        "folder_path = '/content/data_test/25'\n",
        "\n",
        "# Hàm tiền xử lý ảnh\n",
        "def preprocess_image(image_path, target_size=(224, 224)):\n",
        "    img = load_img(image_path, target_size=target_size)  # Resize ảnh\n",
        "    img_array = img_to_array(img)  # Chuyển đổi thành mảng NumPy\n",
        "    img_array = np.expand_dims(img_array, axis=0)  # Thêm chiều batch\n",
        "    img_array = img_array / 255.0  # Chuẩn hóa giá trị pixel về [0, 1]\n",
        "    return img_array\n",
        "\n",
        "# Hàm dự đoán và tính accuracy\n",
        "def predict_from_folder(model, folder_path):\n",
        "    count_is_deepfake = 0\n",
        "    total_files = 0\n",
        "\n",
        "    for file_name in os.listdir(folder_path):\n",
        "        file_path = os.path.join(folder_path, file_name)\n",
        "\n",
        "        # Kiểm tra xem tệp có phải là ảnh không\n",
        "        if file_name.endswith(('.jpg', '.jpeg', '.png')):\n",
        "            total_files += 1\n",
        "            img_array = preprocess_image(file_path)  # Tiền xử lý ảnh\n",
        "            pred = model.predict(img_array)  # Dự đoán\n",
        "            label = np.argmax(pred, axis=1)  # Lấy nhãn (0 hoặc 1)\n",
        "            if label == 1:\n",
        "                count_is_deepfake += 1\n",
        "\n",
        "    # Tính accuracy\n",
        "    if total_files > 0:\n",
        "        accuracy = count_is_deepfake / total_files\n",
        "        print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "    else:\n",
        "        print(\"Không tìm thấy ảnh trong thư mục.\")\n",
        "\n",
        "# # Gọi hàm dự đoán\n",
        "# predict_from_folder(model, folder_path)\n"
      ],
      "metadata": {
        "id": "zAUGmnnlSZq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Đường dẫn thư mục và mô hình\n",
        "folder_path = '/content/82/82'\n",
        "# Gọi hàm dự đoán\n",
        "predict_from_folder(model, folder_path)\n"
      ],
      "metadata": {
        "id": "lmVQPlOLSqcc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "file_path = '/content/Aaron_Eckhart_0001.jpg'\n",
        "img_array = preprocess_image(file_path)  # Tiền xử lý ảnh\n",
        "pred = model.predict(img_array)  # Dự đoán\n",
        "print(pred)\n",
        "label = np.argmax(pred, axis=1)  # Lấy nhãn (0 hoặc 1)\n",
        "if label == 1:\n",
        "    print(\"is_deepfake\")\n",
        "else:\n",
        "    print(\"not_deepfake\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FisjXRSqSvAE",
        "outputId": "0b1c6171-20c7-411a-e65c-b2ef0ef5aa97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
            "[[0.49979946 0.5002005 ]]\n",
            "is_deepfake\n"
          ]
        }
      ]
    }
  ]
}